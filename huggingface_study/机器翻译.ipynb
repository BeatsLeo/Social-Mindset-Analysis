{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers.optimization import get_scheduler\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, MarianModel, AdamW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-ro',use_fast=True)\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "# 编码试算\n",
    "tokenizer.batch_encode_plus([['Hello,this is one sentence','This is another sentence']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(path='wmt16', name='ro-en')\n",
    "\n",
    "# 采样, 数据量太大了跑不动\n",
    "dataset['train'] = dataset['train'].shuffle(1).select(range(20000))\n",
    "dataset['validation'] = dataset['validation'].shuffle(1).select(range(200))\n",
    "dataset['test'] = dataset['test'].shuffle(1).select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "def preprocess_function(data, tokenizer):\n",
    "    # 去除数据中的en和ro\n",
    "    en = [ex['en'] for ex in data['translation']]\n",
    "    ro = [ex['ro'] for ex in data['translation']]\n",
    "\n",
    "    # 源语言直接编码就行了\n",
    "    data = tokenizer.batch_encode_plus(en, max_length=128, truncation=True)\n",
    "\n",
    "    # 目标语言在特殊模块中编码\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        data['labels'] = tokenizer.batch_encode_plus(ro, max_length=128, truncation=True)['input_ids']\n",
    "\n",
    "    return data\n",
    "\n",
    "dataset = dataset.map(function=preprocess_function,\n",
    "                      batched=True,\n",
    "                      batch_size=1000,\n",
    "                      num_proc=4,\n",
    "                      remove_columns=['translation'],\n",
    "                      fn_kwargs={'tokenizer': tokenizer})\n",
    "\n",
    "print(dataset['train'][0])\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据整理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    # 求最长的label\n",
    "    max_length = max([len(i['labels']) for i in data])\n",
    "\n",
    "    # 把所有的label都补pad到最长(便于做成batch)\n",
    "    for i in data:\n",
    "        pads = [-100] * (max_length - len(i['labels']))\n",
    "        i['labels'] = i['labels'] + pads\n",
    "\n",
    "    # 把多个数据整合成一个tensor\n",
    "    data = tokenizer.pad(\n",
    "        encoded_inputs=data, \n",
    "        padding=True, \n",
    "        max_length=None,\n",
    "        pad_to_multiple_of=None,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    # 定义decoder_input_ids\n",
    "    data['decoder_input_ids'] = torch.full_like(data['labels'], tokenizer.get_vocab()['<pad>'], dtype=torch.long)\n",
    "\n",
    "    data['decoder_input_ids'][:, 1:] = data['labels'][:, :-1]\n",
    "    data['decoder_input_ids'][data['decoder_input_ids'] == -100] = tokenizer.get_vocab()['<pad>']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset['train'],\n",
    "    batch_size=8,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "for i, data in enumerate(loader):\n",
    "    break\n",
    "\n",
    "for k, v in data.items():\n",
    "    print(k, v.shape, v[:2])\n",
    "\n",
    "len(loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pretrained = MarianModel.from_pretrained('Helsinki-NLP/opus-mt-en-ro')\n",
    "        self.register_buffer('final_logits_bias', torch.zeros(1, tokenizer.vocab_size)) # 记录在模型的state_dict中, 且不会对其求梯度, 与nn.Parameter相反\n",
    "\n",
    "        self.fc = torch.nn.Linear(512, tokenizer.vocab_size, bias=False)\n",
    "\n",
    "        # 加载预训练模型的参数\n",
    "        parameters = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ro')\n",
    "        self.fc.load_state_dict(parameters.lm_head.state_dict())\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels, decoder_input_ids):\n",
    "        logits = self.pretrained(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "        logits = logits.last_hidden_state\n",
    "\n",
    "        logits = self.fc(logits) + self.final_logits_bias\n",
    "\n",
    "        loss = self.criterion(logits.flatten(end_dim=1), labels.flatten())\n",
    "\n",
    "        return {'loss': loss, 'logits': logits}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型试算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "# 统计参数量\n",
    "print(sum(i.numel() for i in model.parameters()) / 10000)\n",
    "\n",
    "out = model(**data)\n",
    "\n",
    "out['loss'], out['logits'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(path = 'sacrebleu')\n",
    "\n",
    "# 试算\n",
    "metric.compute(predictions=['hello there', 'general kenobi'], references=[['hello there'], ['general kenobi']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    global model\n",
    "    device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr = 2e-5)\n",
    "    scheduler = get_scheduler(name='linear',\n",
    "                              num_warmup_steps=0,\n",
    "                              num_training_steps=len(loader),\n",
    "                              optimizer=optimizer)\n",
    "    \n",
    "    model.train()\n",
    "    for i, data in enumerate(loader):\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].to(device)\n",
    "\n",
    "        out = model(**data)\n",
    "        loss = out['loss']\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # 解决梯度爆炸\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        if(i % 50 == 0):\n",
    "            labels = data['labels'][:, 1:]\n",
    "            out = out['logits'].argmax(dim=2)[:, :-1]\n",
    "\n",
    "            accuracy = (out == labels).sum().item() / labels.numel()\n",
    "\n",
    "            lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "\n",
    "            print(i, loss.item(), lr, accuracy)\n",
    "\n",
    "    model = model.to('cpu')\n",
    "\n",
    "train()\n",
    "torch.save(model, './models/en_gen.model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(metric):\n",
    "    model.eval()\n",
    "\n",
    "    # 数据加载器\n",
    "    loader_test = torch.utils.data.DataLoader(\n",
    "        dataset=dataset['test'], \n",
    "        batch_size=8,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for i, data in enumerate(loader_test):\n",
    "        # 计算\n",
    "        with torch.no_grad():\n",
    "            out = model(**data)\n",
    "        \n",
    "        pred = tokenizer.batch_decode(out['logits'].argmax(dim=2))\n",
    "        label = tokenizer.batch_decode(data['decoder_input_ids'])\n",
    "\n",
    "        predictions.extend(pred)\n",
    "        references.extend(label)\n",
    "\n",
    "        if(i % 2 == 0):\n",
    "            print(i)\n",
    "            input_ids = tokenizer.decode(data['input_ids'][0])\n",
    "\n",
    "            print('input_ids = ', input_ids)\n",
    "            print('pred = ', pred[0])\n",
    "            print('label = ', label[0])\n",
    "\n",
    "        if(i == 10):\n",
    "            break\n",
    "\n",
    "    references = [[j] for j in references]\n",
    "    metrics_out = metric.compute(predictions=predictions, references=references)\n",
    "    print(metrics_out)\n",
    "\n",
    "test(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beatsleo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c34c0319d0a309be29663e51c48d620a80f809bc34ab561fe0a7ac57c4f65c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
