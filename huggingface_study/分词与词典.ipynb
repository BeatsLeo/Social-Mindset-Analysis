{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path = 'bert-base-chinese',\n",
    "    cache_dir = None,\n",
    "    force_download = False,\n",
    ")\n",
    "\n",
    "sents = [\n",
    "    '选择珠江花园的原因就是方便。', \n",
    "    '笔记本的键盘确实爽。', \n",
    "    '房间太小。其他的都一般。',\n",
    "    '今天才知道这书还有第6卷，真有点郁闷。', \n",
    "    '机器背面似乎被撕了张什么标签，残胶还在。',\n",
    "]\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码两个句子\n",
    "out = tokenizer.encode(\n",
    "    text = sents[0], \n",
    "    text_pair = sents[1], \n",
    "\n",
    "    # 当句子长度大于max_length时，截断\n",
    "    truncation = True, \n",
    "\n",
    "    # 一律pad到max_length长度\n",
    "    padding = 'max_length', \n",
    "    add_special_tokens = True, \n",
    "    max_length = 30, \n",
    "    # 可取tf, pt, np, 默认返回list\n",
    "    return_tensors = 'pt',\n",
    ")\n",
    "\n",
    "print(out)\n",
    "tokenizer.decode(out.numpy().tolist()[0]), len(out.numpy().tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增强的编码函数\n",
    "out = tokenizer.encode_plus(\n",
    "    text = sents[0], \n",
    "    text_pair = sents[1], \n",
    "\n",
    "    # 当句子长度大于max_length时，截断\n",
    "    truncation = True, \n",
    "\n",
    "    # 一律pad到max_length长度\n",
    "    padding = 'max_length', \n",
    "    add_special_tokens = True, \n",
    "    max_length = 30, \n",
    "\n",
    "    # 可取tf, pt, np, 默认返回list\n",
    "    return_tensors = None,\n",
    "\n",
    "    # 返回token_type_ids\n",
    "    return_token_type_ids = True,\n",
    "\n",
    "    # 返回attention_mask\n",
    "    return_attention_mask = True, \n",
    "\n",
    "    # 返回special_tokens_mask 特殊符号标识\n",
    "    return_special_tokens_mask = True, \n",
    "\n",
    "    # 返回offset_mapping 标识每个词的起止位置，这个参数只能BertTokenizerFast使用\n",
    "    # return_offset_mapping = True, \n",
    "\n",
    "    # 返回length 标识长度\n",
    "    return_length = True,\n",
    ")\n",
    "\n",
    "print(out)\n",
    "tokenizer.decode(out['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量编码句子\n",
    "out = tokenizer.batch_encode_plus(\n",
    "    # batch_text_or_text_pairs = [sents[0], sents[1]], \n",
    "    # 成对编码\n",
    "    batch_text_or_text_pairs = [(sents[0], sents[1]), (sents[2], sents[3])], \n",
    "\n",
    "    # 当句子长度大于max_length时，截断\n",
    "    truncation = True, \n",
    "\n",
    "    # 一律pad到max_length长度\n",
    "    padding = 'max_length', \n",
    "    add_special_tokens = True, \n",
    "    max_length = 30, \n",
    "\n",
    "    # 可取tf, pt, np, 默认返回list\n",
    "    return_tensors = None,\n",
    "\n",
    "    # 返回token_type_ids\n",
    "    return_token_type_ids = True,\n",
    "\n",
    "    # 返回attention_mask\n",
    "    return_attention_mask = True, \n",
    "\n",
    "    # 返回special_tokens_mask 特殊符号标识\n",
    "    return_special_tokens_mask = True, \n",
    "\n",
    "    # 返回offset_mapping 标识每个词的起止位置，这个参数只能BertTokenizerFast使用\n",
    "    # return_offset_mapping = True, \n",
    "\n",
    "    # 返回length 标识长度\n",
    "    return_length = True,\n",
    ")\n",
    "\n",
    "print(out)\n",
    "tokenizer.decode(out['input_ids'][0]), tokenizer.decode(out['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取字典\n",
    "dic = tokenizer.get_vocab()\n",
    "\n",
    "type(dic), len(dic), '月光' in dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加新词\n",
    "tokenizer.add_tokens(new_tokens=['月光', '希望'])\n",
    "\n",
    "# 添加新符号\n",
    "tokenizer.add_special_tokens({'eos_token': '[EOS]'})\n",
    "\n",
    "dic = tokenizer.get_vocab()\n",
    "\n",
    "type(dic), len(dic), '月光' in dic, dic['月光'], dic['[EOS]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码新添加的词\n",
    "out = tokenizer.encode(\n",
    "    text = '月光的新希望[EOS]', \n",
    "    text_pair = None, \n",
    "\n",
    "    # 当句子长度大于max_length时，截断\n",
    "    truncation = True, \n",
    "\n",
    "    # 一律pad到max_length长度\n",
    "    padding = 'max_length', \n",
    "    add_special_tokens = True, \n",
    "    max_length = 8, \n",
    "    # 可取tf, pt, np, 默认返回list\n",
    "    return_tensors = 'pt',\n",
    ")\n",
    "\n",
    "print(out)\n",
    "tokenizer.decode(out.numpy().tolist()[0]), len(out.numpy().tolist()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('beatsleo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16ea64f9ee948d927ad35fd9dd41586a042d593dc7bf73dbea6b47fb27e81f20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
