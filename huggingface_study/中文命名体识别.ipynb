{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if(torch.cuda.is_available()) else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载编码工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained('hfl/rbt6')\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "# 分词测试\n",
    "tokenizer.batch_encode_plus(\n",
    "    [[\n",
    "        '海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间'', 的', '海', '域', '。'\n",
    "    ],\n",
    "    [\n",
    "        '这', '座', '依', '山'', 傍', '水', '的', '博', '物', '馆', '由', '国', '内', '一', '流', '的', '设', '计', '师', '主', '持', '设', '计', '，', '整', '个', '建', '筑', '群', '精', '美', '而', '恢', '宏', '。'\n",
    "    ]],\n",
    "    truncation=True,        \n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    is_split_into_words=True,   # 告诉分词器已经被分好词了\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        # names = ['0', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']   # labels[单词, 人名开始, 人名中间, 组织开始, 组织中间, 地名开始, 地名中间]\n",
    "\n",
    "        # 在线加载数据集\n",
    "        dataset = load_dataset(path='peoples_daily_ner', split=split)\n",
    "\n",
    "        # 过滤掉太长的句子\n",
    "        def f(data):\n",
    "            return len(data['tokens']) <= 512 -2\n",
    "        \n",
    "        self.dataset = dataset.filter(f)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        tokens = self.dataset[i]['tokens']\n",
    "        labels = self.dataset[i]['ner_tags']\n",
    "        \n",
    "        return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('train')\n",
    "\n",
    "tokens, labels = dataset[0]\n",
    "len(dataset), tokens, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据整理函数\n",
    "def collate_fn(data):\n",
    "    tokens = [i[0] for i in data]\n",
    "    print(tokens)\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    inputs = tokenizer.batch_encode_plus(tokens,\n",
    "                                         truncation=True,\n",
    "                                         padding=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         is_split_into_words=True)\n",
    "    lens = inputs['input_ids'].shape[1]\n",
    "\n",
    "    # 由于每个label长度不等, 将其填充到最长长度(lens)\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = [7] + labels[i]\n",
    "        labels[i] += [7] * lens\n",
    "        labels[i] = labels[i][:lens]\n",
    "    \n",
    "    return inputs, torch.LongTensor(labels)\n",
    "\n",
    "\n",
    "# 数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=4,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看数据样例\n",
    "for i, (inputs, labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader))\n",
    "print(tokenizer.decode(inputs['input_ids'][0]))\n",
    "print(labels[0])\n",
    "\n",
    "for k, v in inputs.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练模型\n",
    "pretrained = AutoModel.from_pretrained('hfl/rbt6')\n",
    "\n",
    "# 统计参数量\n",
    "print(sum(i.numel() for i in pretrained.parameters()) / 10000)\n",
    "\n",
    "# 模型试算\n",
    "#[b, lens] -> [b, lens, 768]\n",
    "pretrained(**inputs).last_hidden_state.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义下游模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义下游模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tuneing = False\n",
    "        self.pretrained = None\n",
    "\n",
    "        self.rnn = torch.nn.GRU(768, 768)\n",
    "        self.fc = torch.nn.Linear(768, 8)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if(self.tuneing):\n",
    "            out = self.pretrained(**inputs).last_hidden_state\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = pretrained(**inputs).last_hidden_state\n",
    "\n",
    "        out, _ = self.rnn(out)\n",
    "        out = self.fc(out).softmax(dim=2)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def fine_tuneing(self, tuneing):\n",
    "        self.tuneing = tuneing\n",
    "        if(tuneing):\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad = True\n",
    "            \n",
    "            pretrained.train()\n",
    "            self.pretrained = pretrained\n",
    "        else:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad = False\n",
    "            \n",
    "            pretrained.eval()\n",
    "            self.pretrained = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "model(inputs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对计算结果和label变形, 并移除pad\n",
    "def reshape_and_remove_pad(outs, labels, attention_mask):\n",
    "    # 变形, 便于计算loss\n",
    "    # [b, lens, 8] -> [b*lens, 8]\n",
    "    outs = outs.reshape(-1, 8)\n",
    "    # [b, lens] -> [b*lens]\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    # 忽略对pad的计算结果\n",
    "    # [b, lens] -> [b*lens - pad]\n",
    "    select = attention_mask.reshape(-1) == 1\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "\n",
    "    return outs, labels\n",
    "\n",
    "reshape_and_remove_pad(torch.randn(2,3,8), torch.ones(2,3), torch.ones(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取正确数量和总数\n",
    "def get_correct_and_total_count(labels, outs):\n",
    "    # [b*lens, 8] -> [b*lens]\n",
    "    outs = outs.argmax(dim=1)\n",
    "    correct = (outs == labels).sum().item()\n",
    "    total = len(labels)\n",
    "\n",
    "    # 计算除了0以外元素的正确率, 因为0太多了, 包括的话, 正确率很容易虚高\n",
    "    select = labels != 0\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "    correct_content = (outs == labels).sum().item()\n",
    "    total_content = len(labels)\n",
    "\n",
    "    return correct, total, correct_content, total_content\n",
    "\n",
    "get_correct_and_total_count(torch.ones(16), torch.randn(16, 8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train(model, pretrained, epoches):\n",
    "    lr = 2e-5 if(model.tuneing) else 5e-4\n",
    "\n",
    "    # 训练\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    model.rnn = model.rnn.to(device)\n",
    "    model.fc = model.fc.to(device)\n",
    "    pretrained = pretrained.to(device)\n",
    "    \n",
    "    for epoch in range(epoches):\n",
    "        for step, (inputs, labels) in enumerate(loader):\n",
    "            # 模型计算\n",
    "            # [b, lens] -> [b, lens, 8]\n",
    "            for i in inputs:\n",
    "                inputs[i] = inputs[i].to(device)\n",
    "            labels = labels.to(device)\n",
    "            outs = model(inputs)\n",
    "\n",
    "            # 对outs和label变形, 并且移除pad\n",
    "            # outs -> [b, lens, 8] -> [c, 8]\n",
    "            # labels -> [b, lens] -> [c]\n",
    "            outs, labels = reshape_and_remove_pad(outs, labels, inputs['attention_mask'])\n",
    "\n",
    "            # 梯度下降\n",
    "            loss = criterion(outs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if(step % 50 == 0):\n",
    "                counts = get_correct_and_total_count(labels, outs)\n",
    "\n",
    "                accuracy = counts[0] / counts[1]\n",
    "                accuracy_content = counts[2] / counts[3]\n",
    "\n",
    "                print(epoch, step, loss.item(), accuracy, accuracy_content)\n",
    "\n",
    "    # torch.save(model, '命名体识别_中文.model')\n",
    "    model = model.cpu()\n",
    "    model.rnn = model.rnn.cpu()\n",
    "    model.fc = model.fc.cpu()\n",
    "    pretrained = pretrained.cpu()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型总参数\n",
    "model.fine_tuneing(False)\n",
    "print(sum(i.numel() for i in model.parameters()) / 10000)\n",
    "train(model, pretrained, 1)\n",
    "\n",
    "model.fine_tuneing(True)\n",
    "print(sum(i.numel() for i in model.parameters()) / 10000)\n",
    "train(model, pretrained, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.9886694829223565 0.953558926487748\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('validation'),\n",
    "                                              batch_size=128,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    correct_content = 0\n",
    "    total_content = 0\n",
    "\n",
    "    for step, (inputs, labels) in enumerate(loader_test):\n",
    "        if(step == 5):\n",
    "            break\n",
    "        print(step)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # [b, lens] -> [b, lens, 8] -> [b, lens]\n",
    "            outs = model(inputs)\n",
    "        \n",
    "        # 对outs和label变形, 并且移除pad\n",
    "        # outs -> [b, lens, 8] -> [c, 8]\n",
    "        # labels -> [b, lens] -> [c]\n",
    "        outs, labels = reshape_and_remove_pad(outs, labels, inputs['attention_mask'])\n",
    "\n",
    "        counts = get_correct_and_total_count(labels, outs)\n",
    "        correct += counts[0]\n",
    "        total += counts[1]\n",
    "        correct_content += counts[2]\n",
    "        total_content += counts[3]\n",
    "\n",
    "    print(correct / total, correct_content / total_content)\n",
    "\n",
    "test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在泸州的小狗就是一个大傻逼 : \n",
      "在 :  0\n",
      "泸 :  地名开始\n",
      "州 :  地名中间\n",
      "的 :  0\n",
      "小 :  0\n",
      "狗 :  0\n",
      "就 :  0\n",
      "是 :  0\n",
      "一 :  0\n",
      "个 :  0\n",
      "大 :  0\n",
      "傻 :  0\n",
      "逼 :  0\n",
      "\n",
      "在泸州的刘天一就是一个大傻逼 : \n",
      "在 :  0\n",
      "泸 :  地名开始\n",
      "州 :  地名中间\n",
      "的 :  0\n",
      "刘 :  姓\n",
      "天 :  名\n",
      "一 :  名\n",
      "就 :  0\n",
      "是 :  0\n",
      "一 :  0\n",
      "个 :  0\n",
      "大 :  0\n",
      "傻 :  0\n",
      "逼 :  0\n"
     ]
    }
   ],
   "source": [
    "sentence1 = '在泸州的小狗就是一个大傻逼'\n",
    "sentence2 = '在泸州的刘天一就是一个大傻逼'\n",
    "names = ['0', '姓', '名', '组织开始', '组织中间', '地名开始', '地名中间', 'NONE']\n",
    "\n",
    "inputs1 = tokenizer.encode_plus(sentence1, return_tensors='pt')\n",
    "inputs2 = tokenizer.encode_plus(sentence2, return_tensors='pt')\n",
    "\n",
    "out1 = model(inputs1)\n",
    "out2 = model(inputs2)\n",
    "\n",
    "print(sentence1, ': ')\n",
    "for n, i in enumerate(out1.argmax(dim=2).tolist()[0][1:-1]):\n",
    "    print(sentence1[n], ': ' , names[i])\n",
    "print()\n",
    "print(sentence2, ': ')\n",
    "for n, i in enumerate(out2.argmax(dim=2).tolist()[0][1:-1]):\n",
    "    print(sentence2[n], ': ' , names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beatsleo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c34c0319d0a309be29663e51c48d620a80f809bc34ab561fe0a7ac57c4f65c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
