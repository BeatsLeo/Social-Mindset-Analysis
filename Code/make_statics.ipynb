{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pynvml\n",
    "import numpy as np\n",
    "from ltp import LTP\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from settings import *\n",
    "from networks.networks import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择剩余显存最大的两个GPU，如果不够则用CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取传入cuda标号的剩余显存\n",
    "def get_cuda_mem_remain(idx):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(idx)\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    total_memory = info.total\n",
    "    used_memory = info.used\n",
    "    remaining_memory = total_memory - used_memory\n",
    "    \n",
    "    return remaining_memory / 1024**2   # 单位: MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 'cpu', 'cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4 # 用于情感分类的批量大小\n",
    "device1 = device2 = 'cpu'   # device1: 情感分类, device2: 文本摘要和命名体识别\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "gpus = torch.cuda.device_count()\n",
    "\n",
    "if(gpus == 1):\n",
    "    device2 = 'cuda'\n",
    "\n",
    "if(gpus > 1):\n",
    "    mem_list = []\n",
    "    for i in range(gpus):\n",
    "        mem_list.append(get_cuda_mem_remain(i))\n",
    "    \n",
    "    if(max(mem_list) > 15000):\n",
    "        batch_size = 64\n",
    "    elif(max(mem_list) > 10000):\n",
    "        batch_size = 32\n",
    "        \n",
    "    device1 = 'cuda:{}'.format(mem_list.index(max(mem_list)))\n",
    "    mem_list[int(device1.split(':')[-1])] = -1\n",
    "    device2 = 'cuda:{}'.format(mem_list.index(max(mem_list)))\n",
    "    \n",
    "\n",
    "pynvml.nvmlShutdown()\n",
    "batch_size, device1, device2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据与模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4_11'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata_path = './rawdata/date_4_11.json'\n",
    "suffix = rawdata_path.split('_')\n",
    "suffix = suffix[-2] + '_' + suffix[-1].split('.')[0]\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "with open(rawdata_path, encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model_root_path = './models/'\n",
    "cls_tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-330M-Sentiment')\n",
    "ner_tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-320M-Chinese', use_fast=False)\n",
    "ner_tokenizer.add_special_tokens({'additional_special_tokens': list(set(POSNAME.values()))})\n",
    "smy_tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Randeng-BART-139M-SUMMARY')\n",
    "\n",
    "ltp = LTP().to(device2)\n",
    "cls_model = torch.load(os.path.join(model_root_path, 'attitude_classify.model')).to(device1).eval()\n",
    "ner_model = torch.load(os.path.join(model_root_path, 'named_entity_recognition.model')).to(device2).eval()\n",
    "smy_model = torch.load(os.path.join(model_root_path, 'text_summary.model')).to(device2).eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计事件和心态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENTS = [] # {'entities': dict, 'post': str, 'summary': str, 'time': str, 'time_hot': {str: {int: int}}}\n",
    "REPEATS = []\n",
    "COMMENTS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断输入事件是否与事件列表中的某个事件重复(重复返回与事件列表中重复的下标, 不重复返回-1)\n",
    "def is_repeat(event, thresh):\n",
    "    # 获取输入事件的key_words\n",
    "    key_words = []\n",
    "    for k in event['entities']:\n",
    "        key_words += event['entities'][k]\n",
    "    key_words = set(key_words)\n",
    "    \n",
    "    for i, e in enumerate(EVENTS):\n",
    "        # 遍历每个事件的key_words\n",
    "        s = []\n",
    "        for k in e['entities']:\n",
    "            s += e['entities'][k]\n",
    "        s = set(s)\n",
    "        # 重合度匹配\n",
    "        ands = key_words & s\n",
    "        if(ands):\n",
    "            lens = min(len(key_words), len(s))\n",
    "            # 重合度大于阈值则重复\n",
    "            if(len(ands) / lens > thresh):\n",
    "                return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将命名体识别模型输出结果转化为命名体类型与key_words映射关系的字典\n",
    "def res2entities(res, summary_inputs):\n",
    "    summary = summary_inputs['input_ids'].cpu()[0]\n",
    "    entities = {ID2ENTITY[i]: [] for i in range(1,12)}\n",
    "    word = ''; last = -1\n",
    "    # 遍历每个字对应的类型\n",
    "    for n, i in enumerate(res.tolist()):\n",
    "        # 0是None, 12是[CLS]或[PAD], 13是[SEP]\n",
    "        if(i == 0 or i == 12 or i == 13):\n",
    "            if(len(word) != 0 and last != -1):\n",
    "                entities[ID2ENTITY[last]].append(word)\n",
    "                word = ''\n",
    "            last = -1\n",
    "            continue\n",
    "        # 出现下一个字的类型与上一个字类型不同\n",
    "        if(i != last and last != -1 and len(word) != 0):\n",
    "            entities[ID2ENTITY[last]].append(word)\n",
    "            word = ''\n",
    "        word += ner_tokenizer.decode(summary[n]) if(ner_tokenizer.decode(summary[n]).replace(' ', '') not in POSVALUES) else ''\n",
    "        last = i\n",
    "    # 处理最后一个字(词)\n",
    "    if(i != 0 and i!= 12 and i != 13):\n",
    "        entities[ID2ENTITY[i]].append(word)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析事件的一组评论心态\n",
    "def att_cls(event, comments):\n",
    "    # 定义小批量处理函数\n",
    "    def collate_fn(event, data):\n",
    "        inputs = cls_tokenizer.batch_encode_plus(\n",
    "                [[event, c['content']] for c in data], return_tensors='pt', truncation=True, max_length=512, padding=True, add_special_tokens=True, return_token_type_ids=True\n",
    "            ).to(device1)\n",
    "\n",
    "        return data, inputs\n",
    "    # 以batch的形式加载评论(避免显存溢出)\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset=comments, batch_size=batch_size, collate_fn=lambda batch: collate_fn(event, batch), shuffle=False, drop_last=False\n",
    "        )\n",
    "    \n",
    "    for batch_comments, inputs in loader:\n",
    "        # 对batch做心态分析\n",
    "        with torch.no_grad():\n",
    "            out = cls_model(**inputs)['cls']\n",
    "            res = out.argmax(dim=1)\n",
    "            # 将该batch的心态分配到对应评论字典中\n",
    "            for c in range(len(batch_comments)):\n",
    "                batch_comments[c]['attitude'] = res[c].item()\n",
    "        \n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_item(item):\n",
    "    # 获取数据\n",
    "    event = item['event'].strip()\n",
    "    post = item['post'].strip()\n",
    "    time = item['time']\n",
    "    ip = item['ip']\n",
    "    thumbs = item['thumbs']\n",
    "    comments = item['comments']\n",
    "    \n",
    "    if(post == '' or post == 'None'):\n",
    "        return\n",
    "    \n",
    "    # 基础热度(点赞数)\n",
    "    hot = thumbs\n",
    "    # 文本摘要\n",
    "    post_inputs = smy_tokenizer.encode_plus(post, return_tensors='pt', truncation=True, max_length=512).to(device2)\n",
    "    summary = smy_tokenizer.decode(smy_model.generate(post_inputs['input_ids'], max_length=128, do_sample=False)[0]).replace('</s>', '').strip()\n",
    "    # 命名体识别\n",
    "    words, pos = ltp.pipeline(summary, tasks = ['cws', 'pos'], return_dict = False) \n",
    "    # 分词\n",
    "    text = ''\n",
    "    for i in range(len(words)):\n",
    "        text = text + POSNAME[pos[i]] + words[i]\n",
    "    #输入模型\n",
    "    summary_inputs = ner_tokenizer.encode_plus(text=text, return_tensors=\"pt\", padding=True, add_special_tokens=True, return_token_type_ids=False).to(device2)\n",
    "    with torch.no_grad():\n",
    "        out = ner_model(**summary_inputs)['cls']\n",
    "        res = out.argmax(dim=1)\n",
    "    # 获取关键词\n",
    "    entities = res2entities(res, summary_inputs)\n",
    "    event = {'entities': entities, 'post':post, 'summary': summary, 'time_hot': {'{}'.format(time): {ip: hot}}}\n",
    "    idx = is_repeat(event, 0.5)\n",
    "    # 对评论情感分析\n",
    "    comments = att_cls(summary, comments)\n",
    "    # 不重复则加入新事件和评论\n",
    "    if(idx == -1):\n",
    "        EVENTS.append(event)\n",
    "        COMMENTS.append(comments)\n",
    "    # 重复则合并\n",
    "    else:\n",
    "        # 合并命名体\n",
    "        for i in EVENTS[idx]['entities']:\n",
    "            EVENTS[idx]['entities'][i] += event['entities'][i]\n",
    "            EVENTS[idx]['entities'][i] = list(set(EVENTS[idx]['entities'][i]))  # 去重\n",
    "        # 如果该时间存在, 合并ip的热度\n",
    "        if(EVENTS[idx]['time_hot'].get(time, None) is not None):\n",
    "            EVENTS[idx]['time_hot'][time][ip] = EVENTS[idx]['time_hot'][time][ip] + hot if(EVENTS[idx]['time_hot'][time].get(ip, None) is not None) else hot\n",
    "        # 不存在, 加入新的时间\n",
    "        else:\n",
    "            EVENTS[idx]['time_hot'].update(event['time_hot'])\n",
    "        \n",
    "        COMMENTS[idx] += comments\n",
    "        REPEATS.append(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [01:30<00:00,  1.69it/s]\n",
      "100%|██████████| 256/256 [02:44<00:00,  1.55it/s]\n",
      "100%|██████████| 45/45 [00:21<00:00,  2.12it/s]\n",
      "100%|██████████| 262/262 [02:50<00:00,  1.53it/s]\n",
      "100%|██████████| 239/239 [02:42<00:00,  1.47it/s]\n",
      "100%|██████████| 106/106 [01:01<00:00,  1.73it/s]\n",
      "100%|██████████| 148/148 [01:19<00:00,  1.87it/s]\n",
      "100%|██████████| 316/316 [03:00<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# 遍历data开始统计事件\n",
    "for key in data.keys():\n",
    "    for item in tqdm(data[key]):\n",
    "        static_item(item)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存事件和评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root = './output_static/events_date_{}'.format(suffix)\n",
    "\n",
    "# 创建文件夹\n",
    "if(not os.path.exists(root)):\n",
    "    os.makedirs(root)\n",
    "    \n",
    "\n",
    "# 保存统计的事件\n",
    "with open('{}/events_date_{}.json'.format(root, suffix), 'w', encoding='utf-8') as f:\n",
    "    json.dump(EVENTS, f, ensure_ascii=False)\n",
    "\n",
    "# 保存统计的心态\n",
    "with open('{}/comments_date_{}.json'.format(root, suffix), 'w', encoding='utf-8') as f:\n",
    "    json.dump(COMMENTS, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存原句与摘要\n",
    "with open(f'{root}/events_post.txt', 'w', encoding='utf-8') as f:\n",
    "    for e in EVENTS:\n",
    "        f.write(e['post'] + ': ' + e['summary'] + '\\n')\n",
    "\n",
    "# 保存重复的事件\n",
    "with open(f'{root}/repeat.txt', 'w', encoding='utf-8') as f:\n",
    "    for e in REPEATS:\n",
    "        f.write(e['post'] + ': ' + e['summary'] + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载事件和评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 加载统计的事件\n",
    "with open('{}/events_date_{}.json'.format(root, suffix), 'r', encoding='utf-8') as f:\n",
    "    EVENTS = json.load(f)\n",
    "\n",
    "# 加载统计的心态\n",
    "with open('{}/comments_date_{}.json'.format(root, suffix), 'r', encoding='utf-8') as f:\n",
    "    COMMENTS = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from ltp import LTP\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取原始数据每条帖子的原话和最新时间\n",
    "def raw_data_post(path):    \n",
    "    data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "    sentences = []; time = ''\n",
    "    for key in data:\n",
    "        for d in data[key]:\n",
    "            # 帖子\n",
    "            time = max(time, d['time'])\n",
    "            sentence = d['post'].strip()\n",
    "            if(sentence != '' and sentence != 'None'):\n",
    "                sentences.append(sentence)\n",
    "    return time, sentences\n",
    "\n",
    "time, posts = raw_data_post(rawdata_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_WORDS = {'time': time, 'comments': [], 'total': []}  # 'comments': {int: dict}, 'total': {str: int}\n",
    "# 加载LTP分词模型\n",
    "ltp = LTP().to(device1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(data, batch_size):\n",
    "    loader = torch.utils.data.DataLoader(dataset=data, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "    words = [[], []]\n",
    "    final_words = []\n",
    "    # 批量获取分词结果和对应词性\n",
    "    for inputs in loader:\n",
    "        with torch.no_grad():\n",
    "            tmp_words = ltp.pipeline(inputs, tasks = ['cws', 'pos'], return_dict = False)\n",
    "            words[0] += tmp_words[0]\n",
    "            words[1] += tmp_words[1]\n",
    "            \n",
    "    # 将词性并列\n",
    "    for w in zip(words[0], words[1]):\n",
    "        final_words += list(zip(w[0], w[1]))\n",
    "\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给posts分词\n",
    "total_seg = segmentation(posts, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载标点符号集\n",
    "with open('./dataset/key_words/stopwords/punctuation.txt', 'r', encoding='utf-8') as f:\n",
    "    punctuation = f.readlines()\n",
    "    punctuation = [w.strip() for w in punctuation]\n",
    "punctuation = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [02:52<00:00,  7.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# 给所有评论分词\n",
    "comments_keywords = []\n",
    "for i in tqdm(range(len(COMMENTS))):\n",
    "    if(COMMENTS[i] == []):\n",
    "        comments_keywords.append([])\n",
    "        continue\n",
    "    # 每个心态的评论单独统计\n",
    "    key_words = {}\n",
    "    groupby = pd.DataFrame(COMMENTS[i]).groupby('attitude')\n",
    "    # 遍历每个心态里面的评论\n",
    "    for attitude, comments in groupby:\n",
    "        comments_seg = segmentation(comments['content'].to_list(), 128)\n",
    "        # 过滤\n",
    "        comments_seg = list(filter(lambda x:x[0] not in punctuation, comments_seg))\n",
    "        # 加入到总的关键词统计中\n",
    "        total_seg += comments_seg\n",
    "        # 加入到每个心态对应的统计中\n",
    "        counted_seg = dict(Counter(comments_seg))\n",
    "        key_words[attitude] = list([[k[0], counted_seg[k]] for k in counted_seg])\n",
    "\n",
    "    comments_keywords.append(key_words)\n",
    "    \n",
    "KEY_WORDS['comments'] = comments_keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在总关键词中过滤停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词\n",
    "with open('./dataset/key_words/stopwords/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.readlines()\n",
    "    stopwords = [w.strip() for w in stopwords]\n",
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计总关键词\n",
    "total_seg = Counter(total_seg)\n",
    "KEY_WORDS['total'] = list([[k[0], k[1], total_seg[k]] for k in dict(total_seg)])\n",
    "# 过滤\n",
    "filted = filter(lambda x: (x[0] not in stopwords) and (x[1] in ['ns', 'n', 'nh', 'v']) and (len(x[0]) > 1), KEY_WORDS['total'])\n",
    "filted = [[i[0], i[2]] for i in filted]\n",
    "filted.sort(key=lambda x: int(x[1]), reverse=True)\n",
    "\n",
    "KEY_WORDS['total'] = filted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存统计的关键词\n",
    "with open('{}/keywords_date_{}.json'.format(root, suffix), 'w', encoding='utf-8') as f:\n",
    "    json.dump(KEY_WORDS, f, ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上传数据库"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = {'ED_ID': -1, 'EVENT_ID': -1, 'COMMENT_ID': -1, 'C_KEYWORD_ID': -1, 'T_KEYWORD_ID': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "suffix = '4_5'\n",
    "root = './output_static/events_date_{}'.format(suffix)\n",
    "\n",
    "# 加载统计的事件\n",
    "with open('{}/events_date_{}.json'.format(root, suffix), 'r', encoding='utf-8') as f:\n",
    "    EVENTS = json.load(f)\n",
    "\n",
    "# 加载统计的心态\n",
    "with open('{}/comments_date_{}.json'.format(root, suffix), 'r', encoding='utf-8') as f:\n",
    "    COMMENTS = json.load(f)\n",
    "    \n",
    "# 加载统计的关键词\n",
    "with open('{}/keywords_date_{}.json'.format(root, suffix), 'r', encoding='utf-8') as f:\n",
    "    KEY_WORDS = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "# 从table表中获取长度为lens的未分配id_name\n",
    "def get_ids(table, id_name, lens, ID, iid):\n",
    "    # 如果有id的起始值, 则往后加\n",
    "    begin_id = ID[iid]\n",
    "    if(begin_id != -1):\n",
    "        ID[iid] += lens\n",
    "        return [i for i in range(begin_id, begin_id+lens)]\n",
    "    \n",
    "    db = pymysql.connect(host='139.155.236.234', user='root', port=3306, password='lishuai110', database='isps')\n",
    "    cursor = db.cursor()\n",
    "    # 获取已有的id\n",
    "    cursor.execute(f'SELECT {id_name} FROM {table}')\n",
    "    exist_ids = cursor.fetchall()\n",
    "    exist_ids = set([i[0] for i in exist_ids])\n",
    "    db.close()\n",
    "    # 分配未有的id\n",
    "    total_ids = set(range(1, lens + max(exist_ids)+1)) if(exist_ids) else set(range(1, lens+1))\n",
    "    use_ids = list(total_ids - exist_ids)\n",
    "    # 如果数据库中ID是连续的, 则直接往后加\n",
    "    if(len(use_ids) == lens):\n",
    "        ID[iid] = use_ids[-1] + 1\n",
    "    return use_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入评论统计表, 并统计点赞和占比最大的心态, 返回SQL语句, 心态, 热度\n",
    "def insert_comments_statistics(comments, event_id):\n",
    "    # 统计心态和点赞\n",
    "    attitudes = [0 for i in range(len(ATTITUDE2ID))]\n",
    "    time_hot = {}\n",
    "    \n",
    "    # 获取comments_id\n",
    "    lens = len(comments)\n",
    "    ids = get_ids('app_comments_statistics', 'comments_id', lens, ID, 'COMMENT_ID')\n",
    "    \n",
    "    # 构造插入数据的SQL语句, 并统计心态\n",
    "    values = \"\"\"VALUES\"\"\"\n",
    "    for i in range(lens):\n",
    "        c = comments[i]\n",
    "        # 统计心态\n",
    "        attitudes[c['attitude']] += 1\n",
    "        # 统计点赞\n",
    "        if(time_hot.get(c['time'], None) is None):\n",
    "            time_hot[c['time']] = {i: 0 for i in range(len(PROVINCE2ID))}\n",
    "        c['thumbs'] = c['thumbs'] if(c['thumbs'] and c['thumbs'] != 'None') else 0\n",
    "        time_hot[c['time']][c['ip']] += c['thumbs']\n",
    "        # 构造SQL\n",
    "        values += f\"\"\"({ids[i]}, \"{c['time']}\", \"{c['content']}\", {c['ip']}, {c['attitude']}, {c['thumbs']}, {event_id}), \"\"\"\n",
    "    \n",
    "    if(values == 'VALUES'):\n",
    "        return '', attitudes.index(max(attitudes)), time_hot\n",
    "    \n",
    "    values = values[:-2] + ';'\n",
    "    # 最终SQL\n",
    "    sql = f\"\"\" INSERT INTO app_comments_statistics {values}\"\"\"\n",
    "    \n",
    "    return sql, attitudes.index(max(attitudes)), time_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将列表转化成字符串(以','隔开)\n",
    "def list2string(lst):\n",
    "    string = ''\n",
    "    for i in lst:\n",
    "        string += str(i) + ', '\n",
    "        \n",
    "    if(string):\n",
    "        string = string[:-2]\n",
    "    return string[:32]\n",
    "\n",
    "# 将列表转化成写入SQL用的字符串\n",
    "def entities2str(entities):\n",
    "    names = ['事物', '机构', '动作', '数量', '人物', '地点', '原因', '物品', '时间', '触发词', '单位']\n",
    "    sql = \"\"\"\"\"\"\n",
    "    for key in names:\n",
    "        sql += '\"' + list2string(entities[key]) + '\", '\n",
    "    return sql[:-2]\n",
    "\n",
    "# 插入事件统计表, 返回SQL语句\n",
    "def insert_event_statistics(event, event_id, attitude):\n",
    "    return f\"\"\" INSERT INTO app_event_statistics \n",
    "                    VALUES({event_id}, \"{event['post'][:512]}\", \"{event['summary']}\", {attitude}, {entities2str(event['entities'])});\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 插入事件分布表, 返回SQL语句\n",
    "def insert_event_distribution(time_hot, hot, event_id):\n",
    "    # 合并time_hot和hot\n",
    "    for time in hot:\n",
    "        if(time_hot.get(time, None) is None):\n",
    "            time_hot[time] = dict(Counter(hot[time]))\n",
    "        else:\n",
    "            hot_time, time_hot_time = Counter(hot[time]), Counter(time_hot[time])\n",
    "            time_hot[time] = dict(hot_time + time_hot_time)\n",
    "        \n",
    "    # 获取id\n",
    "    lens = sum([len(time_hot[i]) for i in time_hot])\n",
    "    ids = get_ids('app_event_distribution', 'id', lens, ID, 'ED_ID')\n",
    "    \n",
    "    # 构造插入数据的SQL语句\n",
    "    values = \"\"\"VALUES\"\"\"; i = 0\n",
    "    for time in time_hot:\n",
    "        for ip in time_hot[time]:\n",
    "            # 构造SQL\n",
    "            values += f\"\"\"({ids[i]}, \"{time}\", {ip}, {time_hot[time][ip]}, {event_id}), \"\"\"\n",
    "            i += 1\n",
    "            \n",
    "    if(values == 'VALUES'):\n",
    "        return ''\n",
    "    \n",
    "    values = values[:-2] + ';'\n",
    "    # 最终SQL\n",
    "    return f\"\"\" INSERT INTO app_event_distribution {values}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入评论关键词表, 返回SQL语句\n",
    "def insert_comment_key_words(c_keywords, time, event_id):\n",
    "    lens = sum([len(c_keywords[i]) for i in c_keywords])\n",
    "    # 获取id\n",
    "    ids = get_ids('app_comments_key_words', 'id', lens, ID, 'C_KEYWORD_ID')\n",
    "    values = \"\"\"VALUES\"\"\"; i = 0\n",
    "    for attitude in c_keywords:\n",
    "        for key_word in c_keywords[attitude]:\n",
    "            # 构造SQL\n",
    "            values += f\"\"\"({ids[i]}, \"{time}\", \"{key_word[0][:16]}\", {key_word[1]}, {attitude}, {event_id}), \"\"\"\n",
    "            i += 1\n",
    "\n",
    "    if(values == 'VALUES'):\n",
    "        return ''\n",
    "        \n",
    "    values = values[:-2] + ';'\n",
    "    # 最终SQL\n",
    "    return f\"\"\" INSERT INTO app_comments_key_words {values}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 62/868 [00:07<01:34,  8.55it/s]\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(1292, \"Incorrect datetime value: '2022--17' for column 'event_time' at row 283\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32me:\\QQDownload\\大创\\交互式舆情分析\\github\\Code\\make_statics.ipynb 单元格 44\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/QQDownload/%E5%A4%A7%E5%88%9B/%E4%BA%A4%E4%BA%92%E5%BC%8F%E8%88%86%E6%83%85%E5%88%86%E6%9E%90/github/Code/make_statics.ipynb#X61sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m cursor \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39mcursor()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/QQDownload/%E5%A4%A7%E5%88%9B/%E4%BA%A4%E4%BA%92%E5%BC%8F%E8%88%86%E6%83%85%E5%88%86%E6%9E%90/github/Code/make_statics.ipynb#X61sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m cursor\u001b[39m.\u001b[39mexecute(e_s_sql)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/QQDownload/%E5%A4%A7%E5%88%9B/%E4%BA%A4%E4%BA%92%E5%BC%8F%E8%88%86%E6%83%85%E5%88%86%E6%9E%90/github/Code/make_statics.ipynb#X61sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m cursor\u001b[39m.\u001b[39;49mexecute(e_d_sql)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/QQDownload/%E5%A4%A7%E5%88%9B/%E4%BA%A4%E4%BA%92%E5%BC%8F%E8%88%86%E6%83%85%E5%88%86%E6%9E%90/github/Code/make_statics.ipynb#X61sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m cursor\u001b[39m.\u001b[39mexecute(c_s_sql)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/QQDownload/%E5%A4%A7%E5%88%9B/%E4%BA%A4%E4%BA%92%E5%BC%8F%E8%88%86%E6%83%85%E5%88%86%E6%9E%90/github/Code/make_statics.ipynb#X61sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m cursor\u001b[39m.\u001b[39mexecute(c_kw_sql)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\beatsleo\\lib\\site-packages\\pymysql\\cursors.py:148\u001b[0m, in \u001b[0;36mCursor.execute\u001b[1;34m(self, query, args)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    146\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmogrify(query, args)\n\u001b[1;32m--> 148\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(query)\n\u001b[0;32m    149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executed \u001b[39m=\u001b[39m query\n\u001b[0;32m    150\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\beatsleo\\lib\\site-packages\\pymysql\\cursors.py:310\u001b[0m, in \u001b[0;36mCursor._query\u001b[1;34m(self, q)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_executed \u001b[39m=\u001b[39m q\n\u001b[0;32m    309\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_result()\n\u001b[1;32m--> 310\u001b[0m conn\u001b[39m.\u001b[39;49mquery(q)\n\u001b[0;32m    311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_get_result()\n\u001b[0;32m    312\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrowcount\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\beatsleo\\lib\\site-packages\\pymysql\\connections.py:548\u001b[0m, in \u001b[0;36mConnection.query\u001b[1;34m(self, sql, unbuffered)\u001b[0m\n\u001b[0;32m    546\u001b[0m     sql \u001b[39m=\u001b[39m sql\u001b[39m.\u001b[39mencode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding, \u001b[39m\"\u001b[39m\u001b[39msurrogateescape\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    547\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute_command(COMMAND\u001b[39m.\u001b[39mCOM_QUERY, sql)\n\u001b[1;32m--> 548\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_affected_rows \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_query_result(unbuffered\u001b[39m=\u001b[39;49munbuffered)\n\u001b[0;32m    549\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_affected_rows\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\beatsleo\\lib\\site-packages\\pymysql\\connections.py:775\u001b[0m, in \u001b[0;36mConnection._read_query_result\u001b[1;34m(self, unbuffered)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    774\u001b[0m     result \u001b[39m=\u001b[39m MySQLResult(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 775\u001b[0m     result\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m    776\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m result\n\u001b[0;32m    777\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mserver_status \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\beatsleo\\lib\\site-packages\\pymysql\\connections.py:1156\u001b[0m, in \u001b[0;36mMySQLResult.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1155\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1156\u001b[0m         first_packet \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnection\u001b[39m.\u001b[39;49m_read_packet()\n\u001b[0;32m   1158\u001b[0m         \u001b[39mif\u001b[39;00m first_packet\u001b[39m.\u001b[39mis_ok_packet():\n\u001b[0;32m   1159\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_ok_packet(first_packet)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\beatsleo\\lib\\site-packages\\pymysql\\connections.py:725\u001b[0m, in \u001b[0;36mConnection._read_packet\u001b[1;34m(self, packet_type)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39munbuffered_active \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    724\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39munbuffered_active \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 725\u001b[0m     packet\u001b[39m.\u001b[39;49mraise_for_error()\n\u001b[0;32m    726\u001b[0m \u001b[39mreturn\u001b[39;00m packet\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\beatsleo\\lib\\site-packages\\pymysql\\protocol.py:221\u001b[0m, in \u001b[0;36mMysqlPacket.raise_for_error\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39mif\u001b[39;00m DEBUG:\n\u001b[0;32m    220\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39merrno =\u001b[39m\u001b[39m\"\u001b[39m, errno)\n\u001b[1;32m--> 221\u001b[0m err\u001b[39m.\u001b[39;49mraise_mysql_exception(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\beatsleo\\lib\\site-packages\\pymysql\\err.py:143\u001b[0m, in \u001b[0;36mraise_mysql_exception\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m errorclass \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m     errorclass \u001b[39m=\u001b[39m InternalError \u001b[39mif\u001b[39;00m errno \u001b[39m<\u001b[39m \u001b[39m1000\u001b[39m \u001b[39melse\u001b[39;00m OperationalError\n\u001b[1;32m--> 143\u001b[0m \u001b[39mraise\u001b[39;00m errorclass(errno, errval)\n",
      "\u001b[1;31mOperationalError\u001b[0m: (1292, \"Incorrect datetime value: '2022--17' for column 'event_time' at row 283\")"
     ]
    }
   ],
   "source": [
    "# 插入评论统计表, 事件统计表, 事件分布表, 评论关键词表\n",
    "lens = len(EVENTS)\n",
    "ids = get_ids('app_event_statistics', 'event_id', lens, ID, 'EVENT_ID')\n",
    "\n",
    "# 构造SQL\n",
    "for e in tqdm(range(lens)):\n",
    "    c_s_sql, attitude, hot = insert_comments_statistics(COMMENTS[e], ids[e])\n",
    "    e_s_sql = insert_event_statistics(EVENTS[e], ids[e], attitude)\n",
    "    e_d_sql = insert_event_distribution(EVENTS[e]['time_hot'], hot, ids[e])\n",
    "    c_kw_sql = insert_comment_key_words(KEY_WORDS['comments'][e], KEY_WORDS['time'], ids[e])\n",
    "    if(c_s_sql == '' or e_d_sql == '' or e_s_sql == '' or c_kw_sql == ''):\n",
    "        continue\n",
    "    \n",
    "    # 往数据库里插入\n",
    "    db = pymysql.connect(host='139.155.236.234', user='root', port=3306, password='lishuai110', database='isps')\n",
    "    cursor = db.cursor()\n",
    "    try:\n",
    "        cursor.execute(e_s_sql)\n",
    "        cursor.execute(e_d_sql)\n",
    "        cursor.execute(c_s_sql)\n",
    "        cursor.execute(c_kw_sql)\n",
    "    except pymysql.OperationalError:\n",
    "        continue\n",
    "    db.commit()\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入总关键词表\n",
    "def insert_total_key_words(key_words, time):\n",
    "    # 获取id\n",
    "    lens = len(key_words)\n",
    "    ids = get_ids('app_event_key_words', 'id', lens, ID, 'T_KEYWORD_ID')\n",
    "    values = \"\"\"VALUES\"\"\"\n",
    "    for i in range(lens):\n",
    "        # 构造SQL\n",
    "        values += f\"\"\"({ids[i]}, \"{time}\", \"{key_words[i][0][:16]}\", {key_words[i][1]}), \"\"\"\n",
    "    \n",
    "    if(values == 'VALUES'):\n",
    "        return ''\n",
    "        \n",
    "    values = values[:-2] + ';'\n",
    "    # 最终SQL\n",
    "    return f\"\"\" INSERT INTO app_event_key_words {values}\"\"\"\n",
    "\n",
    "# 往数据库里插入\n",
    "db = pymysql.connect(host='139.155.236.234', user='root', port=3306, password='lishuai110', database='isps')\n",
    "cursor = db.cursor()\n",
    "t_kw_sql = insert_total_key_words(KEY_WORDS['total'], KEY_WORDS['time'])\n",
    "if(t_kw_sql):\n",
    "    cursor.execute(t_kw_sql)\n",
    "db.commit()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8486794328cfcc2cb332d4b8a0be7eefe604ab39976dd3551ed661a77c4a469b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
