{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pynvml\n",
    "import numpy as np\n",
    "from ltp import LTP\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from settings import *\n",
    "from networks.networks import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择剩余显存最大的两个GPU，如果不够则用CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取传入cuda标号的剩余显存\n",
    "def get_cuda_mem_remain(idx):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(idx)\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    total_memory = info.total\n",
    "    used_memory = info.used\n",
    "    remaining_memory = total_memory - used_memory\n",
    "    \n",
    "    return remaining_memory / 1024**2   # 单位: MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # 用于情感分类的批量大小\n",
    "device1 = device2 = 'cpu'   # device1: 情感分类, device2: 文本摘要和命名体识别\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "gpus = torch.cuda.device_count()\n",
    "\n",
    "if(gpus == 1):\n",
    "    device2 = 'cuda'\n",
    "\n",
    "if(gpus > 1):\n",
    "    mem_list = []\n",
    "    for i in range(gpus):\n",
    "        mem_list.append(get_cuda_mem_remain(i))\n",
    "    \n",
    "    if(max(mem_list) > 15000):\n",
    "        batch_size = 64\n",
    "    elif(max(mem_list) > 10000):\n",
    "        batch_size = 32\n",
    "        \n",
    "    device1 = 'cuda:{}'.format(mem_list.index(max(mem_list)))\n",
    "    mem_list[int(device1.split(':')[-1])] = -1\n",
    "    device2 = 'cuda:{}'.format(mem_list.index(max(mem_list)))\n",
    "    \n",
    "\n",
    "pynvml.nvmlShutdown()\n",
    "batch_size, device1, device2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据与模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_path = './rawdata/weibo_2_8.json'\n",
    "suffix = rawdata_path.split('_')\n",
    "suffix = suffix[-2] + '_' + suffix[-1].split('.')[0]\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "with open(rawdata_path, encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model_root_path = './models/'\n",
    "cls_tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-330M-Sentiment')\n",
    "ner_tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-DeBERTa-v2-320M-Chinese', use_fast=False)\n",
    "ner_tokenizer.add_special_tokens({'additional_special_tokens': list(set(POSNAME.values()))})\n",
    "smy_tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Randeng-BART-139M-SUMMARY')\n",
    "\n",
    "ltp = LTP().to(device2)\n",
    "cls_model = torch.load(os.path.join(model_root_path, 'attitude_classify.model')).to(device1).eval()\n",
    "ner_model = torch.load(os.path.join(model_root_path, 'named_entity_recognition.model')).to(device2).eval()\n",
    "smy_model = torch.load(os.path.join(model_root_path, 'text_summary.model')).to(device2).eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计事件和心态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENTS = [] # {'entities': dict, 'post': str, 'summary': str, 'time': str, 'time_hot': {str: {int: int}}}\n",
    "REPEATS = []\n",
    "COMMENTS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断输入事件是否与事件列表中的某个事件重复(重复返回与事件列表中重复的下标, 不重复返回-1)\n",
    "def is_repeat(event, thresh):\n",
    "    # 获取输入事件的key_words\n",
    "    key_words = []\n",
    "    for k in event['entities']:\n",
    "        key_words += event['entities'][k]\n",
    "    key_words = set(key_words)\n",
    "    \n",
    "    for i, e in enumerate(EVENTS):\n",
    "        # 遍历每个事件的key_words\n",
    "        s = []\n",
    "        for k in e['entities']:\n",
    "            s += e['entities'][k]\n",
    "        s = set(s)\n",
    "        # 重合度匹配\n",
    "        ands = key_words & s\n",
    "        if(ands):\n",
    "            lens = min(len(key_words), len(s))\n",
    "            # 重合度大于阈值则重复\n",
    "            if(len(ands) / lens > thresh):\n",
    "                return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将命名体识别模型输出结果转化为命名体类型与key_words映射关系的字典\n",
    "def res2entities(res, summary_inputs):\n",
    "    summary = summary_inputs['input_ids'].cpu()[0]\n",
    "    entities = {ID2ENTITY[i]: [] for i in range(1,12)}\n",
    "    word = ''; last = -1\n",
    "    # 遍历每个字对应的类型\n",
    "    for n, i in enumerate(res.tolist()):\n",
    "        # 0是None, 12是[CLS]或[PAD], 13是[SEP]\n",
    "        if(i == 0 or i == 12 or i == 13):\n",
    "            if(len(word) != 0 and last != -1):\n",
    "                entities[ID2ENTITY[last]].append(word)\n",
    "                word = ''\n",
    "            last = -1\n",
    "            continue\n",
    "        # 出现下一个字的类型与上一个字类型不同\n",
    "        if(i != last and last != -1 and len(word) != 0):\n",
    "            entities[ID2ENTITY[last]].append(word)\n",
    "            word = ''\n",
    "            \n",
    "        word += ner_tokenizer.decode(summary[n]) if(ner_tokenizer.decode(summary[n]).replace(' ', '') not in POSVALUES) else ''\n",
    "        last = i\n",
    "    # 处理最后一个字(词)\n",
    "    if(i != 0 and i!= 12 and i != 13):\n",
    "        entities[ID2ENTITY[i]].append(word)    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析事件的一组评论心态\n",
    "def att_cls(event, comments):\n",
    "    # 定义小批量处理函数\n",
    "    def collate_fn(event, data):\n",
    "        inputs = cls_tokenizer.batch_encode_plus(\n",
    "                [[event, c['content']] for c in data], return_tensors='pt', truncation=True, max_length=512, padding=True, add_special_tokens=True, return_token_type_ids=True\n",
    "            ).to(device1)\n",
    "\n",
    "        return data, inputs\n",
    "    # 以batch的形式加载评论(避免显存溢出)\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset=comments, batch_size=batch_size, collate_fn=lambda batch: collate_fn(event, batch), shuffle=False, drop_last=False\n",
    "        )\n",
    "    \n",
    "    for batch_comments, inputs in loader:\n",
    "        # 对batch做心态分析\n",
    "        with torch.no_grad():\n",
    "            out = cls_model(**inputs)['cls']\n",
    "            res = out.argmax(dim=1)\n",
    "            # 将该batch的心态分配到对应评论字典中\n",
    "            for c in range(len(batch_comments)):\n",
    "                batch_comments[c]['attitude'] = res[c].item()\n",
    "        \n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_item(item):\n",
    "    # 获取数据\n",
    "    event = item['event'].strip()\n",
    "    post = item['post'].strip()\n",
    "    time = item['time']\n",
    "    ip = item['ip']\n",
    "    thumbs = item['thumbs']\n",
    "    comments = item['comments']\n",
    "    \n",
    "    if(post == '' or post == 'None'):\n",
    "        return\n",
    "    \n",
    "    # 基础热度(点赞数)\n",
    "    hot = thumbs\n",
    "    # 文本摘要\n",
    "    post_inputs = smy_tokenizer.encode_plus(post, return_tensors='pt').to(device2)\n",
    "    summary = smy_tokenizer.decode(smy_model.generate(post_inputs['input_ids'], max_length=128, do_sample=False)[0]).replace('</s>', '').strip()\n",
    "    # 命名体识别\n",
    "    words, pos = ltp.pipeline(summary, tasks = ['cws', 'pos'], return_dict = False) \n",
    "    # 分词\n",
    "    text = ''\n",
    "    for i in range(len(words)):\n",
    "        text = text + POSNAME[pos[i]] + words[i]\n",
    "    #输入模型\n",
    "    summary_inputs = ner_tokenizer.encode_plus(text=text, return_tensors=\"pt\", padding=True, add_special_tokens=True, return_token_type_ids=False).to(device2)\n",
    "    with torch.no_grad():\n",
    "        out = ner_model(**summary_inputs)['cls']\n",
    "        res = out.argmax(dim=1)\n",
    "    # 获取关键词\n",
    "    entities = res2entities(res, summary_inputs)\n",
    "    event = {'entities': entities, 'post':post, 'summary': summary, 'time_hot': {'{}'.format(time): {ip: hot}}}\n",
    "    idx = is_repeat(event, 0.5)\n",
    "    # 对评论情感分析\n",
    "    comments = att_cls(summary, comments)\n",
    "    # 不重复则加入新事件和评论\n",
    "    if(idx == -1):\n",
    "        EVENTS.append(event)\n",
    "        COMMENTS.append(comments)\n",
    "    # 重复则合并\n",
    "    else:\n",
    "        EVENTS[idx]['entities'].update(event['entities'])\n",
    "        # 如果该时间存在, 合并ip的热度\n",
    "        if(EVENTS[idx]['time_hot'].get(time, None) is not None):\n",
    "            EVENTS[idx]['time_hot'][time][ip] = EVENTS[idx]['time_hot'][time][ip] + hot if(EVENTS[idx]['time_hot'][time].get(ip, None) is not None) else hot\n",
    "        # 不存在, 加入新的时间\n",
    "        else:\n",
    "            EVENTS[idx]['time_hot'].update(event['time_hot'])\n",
    "        \n",
    "        COMMENTS[idx] += comments\n",
    "        REPEATS.append(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历data开始统计事件\n",
    "for key in data.keys():\n",
    "    for item in tqdm(data[key]):\n",
    "        static_item(item)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存事件和评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存统计的事件\n",
    "with open('./output_static/events_weibo_{}.json'.format(suffix), 'w', encoding='utf-8') as f:\n",
    "    json.dump(EVENTS, f, ensure_ascii=False)\n",
    "\n",
    "# 保存统计的心态\n",
    "with open('./output_static/comments_weibo_{}.json'.format(suffix), 'w', encoding='utf-8') as f:\n",
    "    json.dump(COMMENTS, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存原句与摘要\n",
    "with open('./output_static/events_post.txt', 'w', encoding='utf-8') as f:\n",
    "    for e in EVENTS:\n",
    "        f.write(e['post'] + ': ' + e['summary'] + '\\n')\n",
    "\n",
    "# 保存重复的事件\n",
    "with open('./output_static/repeat.txt', 'w', encoding='utf-8') as f:\n",
    "    for e in REPEATS:\n",
    "        f.write(e['post'] + ': ' + e['summary'] + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载事件和评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 加载统计的事件\n",
    "with open('./output_static/events_weibo_{}.json'.format(suffix), 'r', encoding='utf-8') as f:\n",
    "    EVENTS = json.load(f)\n",
    "\n",
    "# 加载统计的心态\n",
    "with open('./output_static/comments_weibo_{}.json'.format(suffix), 'r', encoding='utf-8') as f:\n",
    "    COMMENTS = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from ltp import LTP\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['热门'][0]['time']\n",
    "'2022-03-06' > ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取原始数据每条帖子的原话和最新时间\n",
    "def raw_data_post(path):    \n",
    "    data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "    sentences = []; time = ''\n",
    "    for key in data:\n",
    "        for d in data[key]:\n",
    "            # 帖子\n",
    "            time = max(time, d['time'])\n",
    "            sentence = d['post'].strip()\n",
    "            if(sentence != '' and sentence != 'None'):\n",
    "                sentences.append(sentence)\n",
    "    return time, sentences\n",
    "\n",
    "time, posts = raw_data_post(rawdata_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_WORDS = {'time': time, 'comments': [], 'total': []}  # 'comments': {int: dict}, 'total': {str: int}\n",
    "# 加载LTP分词模型\n",
    "ltp = LTP().to(device1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(data, batch_size):\n",
    "    loader = torch.utils.data.DataLoader(dataset=data, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "    words = [[], []]\n",
    "    final_words = []\n",
    "    # 批量获取分词结果和对应词性\n",
    "    for inputs in loader:\n",
    "        with torch.no_grad():\n",
    "            tmp_words = ltp.pipeline(inputs, tasks = ['cws', 'pos'], return_dict = False)\n",
    "            words[0] += tmp_words[0]\n",
    "            words[1] += tmp_words[1]\n",
    "            \n",
    "    # 将词性并列\n",
    "    for w in zip(words[0], words[1]):\n",
    "        final_words += list(zip(w[0], w[1]))\n",
    "\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给posts分词\n",
    "total_seg = segmentation(posts, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给所有评论分词\n",
    "comments_keywords = []\n",
    "for i in tqdm(range(len(COMMENTS))):\n",
    "    if(COMMENTS[i] == []):\n",
    "        comments_keywords.append([])\n",
    "        continue\n",
    "    # 每个心态的评论单独统计\n",
    "    key_words = {}\n",
    "    groupby = pd.DataFrame(COMMENTS[i]).groupby('attitude')\n",
    "    # 遍历每个心态里面的评论\n",
    "    for attitude, comments in groupby:\n",
    "        comments_seg = segmentation(comments['content'].to_list(), 128)\n",
    "        # 加入到总的关键词统计中\n",
    "        total_seg += comments_seg\n",
    "        # 加入到每个心态对应的统计中\n",
    "        counted_seg = dict(Counter(comments_seg))\n",
    "        key_words[attitude] = list([[k[0], counted_seg[k]] for k in counted_seg])\n",
    "\n",
    "    comments_keywords.append(key_words)\n",
    "    \n",
    "KEY_WORDS['comments'] = comments_keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在总关键词中过滤停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词\n",
    "with open('./dataset/key_words/stopwords/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.readlines()\n",
    "    stopwords = [w.strip() for w in stopwords]\n",
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计总关键词\n",
    "total_seg = Counter(total_seg)\n",
    "KEY_WORDS['total'] = list([[k[0], k[1], total_seg[k]] for k in dict(total_seg)])\n",
    "# 过滤\n",
    "filted = filter(lambda x: (x[0] not in stopwords) and (x[1] in ['ns', 'n', 'nh', 'v']) and (len(x[0]) > 1), KEY_WORDS['total'])\n",
    "filted = [[i[0], i[2]] for i in filted]\n",
    "filted.sort(key=lambda x: int(x[1]), reverse=True)\n",
    "\n",
    "KEY_WORDS['total'] = filted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存统计的关键词\n",
    "with open('./output_static/keywords_weibo_{}.json'.format(suffix), 'w', encoding='utf-8') as f:\n",
    "    json.dump(KEY_WORDS, f, ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上传数据库"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "suffix = '2_8'\n",
    "\n",
    "# 加载统计的事件\n",
    "with open('./output_static/events_{}.json'.format(suffix), 'r', encoding='utf-8') as f:\n",
    "    EVENTS = json.load(f)\n",
    "\n",
    "# 加载统计的心态\n",
    "with open('./output_static/comments_{}.json'.format(suffix), 'r', encoding='utf-8') as f:\n",
    "    COMMENTS = json.load(f)\n",
    "    \n",
    "# 加载统计的关键词\n",
    "with open('./output_static/keywords_{}.json'.format(suffix), 'r', encoding='utf-8') as f:\n",
    "    KEY_WORDS = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = {'ED_ID': -1, 'EVENT_ID': -1, 'COMMENT_ID': -1, 'C_KEYWORD_ID': -1, 'T_KEYWORD_ID': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "# 从table表中获取长度为lens的未分配id_name\n",
    "def get_ids(table, id_name, lens, ID, iid):\n",
    "    # 如果有id的起始值, 则往后加\n",
    "    begin_id = ID[iid]\n",
    "    if(begin_id != -1):\n",
    "        ID[iid] += lens\n",
    "        return [i for i in range(begin_id, begin_id+lens)]\n",
    "    \n",
    "    db = pymysql.connect(host='139.155.236.234', user='root', port=3306, password='lishuai110', database='isps')\n",
    "    cursor = db.cursor()\n",
    "    # 获取已有的id\n",
    "    cursor.execute(f'SELECT {id_name} FROM {table}')\n",
    "    exist_ids = cursor.fetchall()\n",
    "    exist_ids = set([i[0] for i in exist_ids])\n",
    "    db.close()\n",
    "    # 分配未有的id\n",
    "    total_ids = set(range(1, lens + max(exist_ids)+1)) if(exist_ids) else set(range(1, lens+1))\n",
    "    use_ids = list(total_ids - exist_ids)\n",
    "    # 如果数据库中ID是连续的, 则直接往后加\n",
    "    if(len(use_ids) == lens):\n",
    "        ID[iid] = use_ids[-1] + 1\n",
    "    return use_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入评论统计表, 并统计点赞和占比最大的心态, 返回SQL语句, 心态, 热度\n",
    "def insert_comments_statistics(comments, event_id):\n",
    "    # 统计心态和点赞\n",
    "    attitudes = [0 for i in range(len(ATTITUDE2ID))]\n",
    "    time_hot = {}\n",
    "    \n",
    "    # 获取comments_id\n",
    "    lens = len(comments)\n",
    "    ids = get_ids('app_comments_statistics', 'comments_id', lens, ID, 'COMMENT_ID')\n",
    "    \n",
    "    # 构造插入数据的SQL语句, 并统计心态\n",
    "    values = \"\"\"VALUES\"\"\"\n",
    "    for i in range(lens):\n",
    "        c = comments[i]\n",
    "        # 统计心态\n",
    "        attitudes[c['attitude']] += 1\n",
    "        # 统计点赞\n",
    "        if(time_hot.get(c['time'], None) is None):\n",
    "            time_hot[c['time']] = {i: 0 for i in range(len(PROVINCE2ID))}\n",
    "        c['thumbs'] = c['thumbs'] if(c['thumbs'] and c['thumbs'] != 'None') else 0\n",
    "        time_hot[c['time']][c['ip']] += c['thumbs']\n",
    "        # 构造SQL\n",
    "        values += f\"\"\"({ids[i]}, \"{c['time']}\", \"{c['content']}\", {c['ip']}, {c['attitude']}, {c['thumbs']}, {event_id}), \"\"\"\n",
    "    \n",
    "    if(values == 'VALUES'):\n",
    "        return '', attitudes.index(max(attitudes)), time_hot\n",
    "    \n",
    "    values = values[:-2] + ';'\n",
    "    # 最终SQL\n",
    "    sql = f\"\"\" INSERT INTO app_comments_statistics {values}\"\"\"\n",
    "    \n",
    "    return sql, attitudes.index(max(attitudes)), time_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将列表转化成字符串(以','隔开)\n",
    "def list2string(lst):\n",
    "    string = ''\n",
    "    for i in lst:\n",
    "        string += str(i) + ', '\n",
    "        \n",
    "    if(string):\n",
    "        string = string[:-2]\n",
    "    return string[:32]\n",
    "\n",
    "# 将列表转化成写入SQL用的字符串\n",
    "def entities2str(entities):\n",
    "    names = ['事物', '机构', '动作', '数量', '人物', '地点', '原因', '物品', '时间', '触发词', '单位']\n",
    "    sql = \"\"\"\"\"\"\n",
    "    for key in names:\n",
    "        sql += '\"' + list2string(entities[key]) + '\", '\n",
    "    return sql[:-2]\n",
    "\n",
    "# 插入事件统计表, 返回SQL语句\n",
    "def insert_event_statistics(event, event_id, attitude):\n",
    "    return f\"\"\" INSERT INTO app_event_statistics \n",
    "                    VALUES({event_id}, \"{event['post'][:512]}\", \"{event['summary']}\", {attitude}, {entities2str(event['entities'])});\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 插入事件分布表, 返回SQL语句\n",
    "def insert_event_distribution(time_hot, hot, event_id):\n",
    "    # 合并time_hot和hot\n",
    "    for time in hot:\n",
    "        if(time_hot.get(time, None) is None):\n",
    "            time_hot[time] = dict(Counter(hot[time]))\n",
    "        else:\n",
    "            hot_time, time_hot_time = Counter(hot[time]), Counter(time_hot[time])\n",
    "            time_hot[time] = dict(hot_time + time_hot_time)\n",
    "        \n",
    "    # 获取id\n",
    "    lens = sum([len(time_hot[i]) for i in time_hot])\n",
    "    ids = get_ids('app_event_distribution', 'id', lens, ID, 'ED_ID')\n",
    "    \n",
    "    # 构造插入数据的SQL语句\n",
    "    values = \"\"\"VALUES\"\"\"; i = 0\n",
    "    for time in time_hot:\n",
    "        for ip in time_hot[time]:\n",
    "            # 构造SQL\n",
    "            values += f\"\"\"({ids[i]}, \"{time}\", {ip}, {time_hot[time][ip]}, {event_id}), \"\"\"\n",
    "            i += 1\n",
    "            \n",
    "    if(values == 'VALUES'):\n",
    "        return ''\n",
    "    \n",
    "    values = values[:-2] + ';'\n",
    "    # 最终SQL\n",
    "    return f\"\"\" INSERT INTO app_event_distribution {values}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入评论关键词表, 返回SQL语句\n",
    "def insert_comment_key_words(c_keywords, time, event_id):\n",
    "    lens = sum([len(c_keywords[i]) for i in c_keywords])\n",
    "    # 获取id\n",
    "    ids = get_ids('app_comments_key_words', 'id', lens, ID, 'C_KEYWORD_ID')\n",
    "    values = \"\"\"VALUES\"\"\"; i = 0\n",
    "    for attitude in c_keywords:\n",
    "        for key_word in c_keywords[attitude]:\n",
    "            # 构造SQL\n",
    "            values += f\"\"\"({ids[i]}, \"{time}\", \"{key_word[0][:16]}\", {key_word[1]}, {attitude}, {event_id}), \"\"\"\n",
    "            i += 1\n",
    "\n",
    "    if(values == 'VALUES'):\n",
    "        return ''\n",
    "        \n",
    "    values = values[:-2] + ';'\n",
    "    # 最终SQL\n",
    "    return f\"\"\" INSERT INTO app_comments_key_words {values}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入评论统计表, 事件统计表, 事件分布表, 评论关键词表\n",
    "lens = len(EVENTS)\n",
    "ids = get_ids('app_event_statistics', 'event_id', lens, ID, 'EVENT_ID')\n",
    "\n",
    "# 构造SQL\n",
    "for e in tqdm(range(lens)):\n",
    "    c_s_sql, attitude, hot = insert_comments_statistics(COMMENTS[e], ids[e])\n",
    "    e_s_sql = insert_event_statistics(EVENTS[e], ids[e], attitude)\n",
    "    e_d_sql = insert_event_distribution(EVENTS[e]['time_hot'], hot, ids[e])\n",
    "    c_kw_sql = insert_comment_key_words(KEY_WORDS['comments'][e], KEY_WORDS['time'], ids[e])\n",
    "    if(c_s_sql == '' or e_d_sql == '' or e_s_sql == '' or c_kw_sql == ''):\n",
    "        continue\n",
    "    \n",
    "    # 往数据库里插入\n",
    "    db = pymysql.connect(host='139.155.236.234', user='root', port=3306, password='lishuai110', database='isps')\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(e_s_sql)\n",
    "    cursor.execute(e_d_sql)\n",
    "    cursor.execute(c_s_sql)\n",
    "    cursor.execute(c_kw_sql)\n",
    "    db.commit()\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入总关键词表\n",
    "def insert_total_key_words(key_words, time):\n",
    "    # 获取id\n",
    "    lens = len(key_words)\n",
    "    ids = get_ids('app_event_key_words', 'id', lens, ID, 'T_KEYWORD_ID')\n",
    "    values = \"\"\"VALUES\"\"\"\n",
    "    for i in range(lens):\n",
    "        # 构造SQL\n",
    "        values += f\"\"\"({ids[i]}, \"{time}\", \"{key_words[i][0][:16]}\", {key_words[i][1]}), \"\"\"\n",
    "    \n",
    "    if(values == 'VALUES'):\n",
    "        return ''\n",
    "        \n",
    "    values = values[:-2] + ';'\n",
    "    # 最终SQL\n",
    "    return f\"\"\" INSERT INTO app_event_key_words {values}\"\"\"\n",
    "\n",
    "# 往数据库里插入\n",
    "db = pymysql.connect(host='139.155.236.234', user='root', port=3306, password='lishuai110', database='isps')\n",
    "cursor = db.cursor()\n",
    "t_kw_sql = insert_total_key_words(KEY_WORDS['total'], KEY_WORDS['time'])\n",
    "if(t_kw_sql):\n",
    "    cursor.execute(t_kw_sql)\n",
    "db.commit()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8486794328cfcc2cb332d4b8a0be7eefe604ab39976dd3551ed661a77c4a469b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
