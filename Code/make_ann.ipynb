{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import openpyxl\n",
    "from openpyxl.styles import Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data(path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "data = get_raw_data('./rawdata/weibo3.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本摘要数据导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "posts = []\n",
    "for i in data:\n",
    "    for d in data[i]:\n",
    "        events.append(d['event'])\n",
    "        posts.append(d['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_excel(posts, events, output_file_name):\n",
    "    \"\"\"\n",
    "    将数据写入xlsx文件\n",
    "    \"\"\"\n",
    "    if not output_file_name.endswith('.xlsx'):\n",
    "        output_file_name += '.xlsx'\n",
    " \n",
    "    # 创建一个workbook对象，而且会在workbook中至少创建一个表worksheet\n",
    "    wb = openpyxl.Workbook()\n",
    "    # 获取当前活跃的worksheet,默认就是第一个worksheet\n",
    "    ws = wb.active\n",
    "    align = Alignment(horizontal='center', vertical='center', wrap_text=True)\n",
    "    ws.column_dimensions['A'].width = 40.0\n",
    "    ws.column_dimensions['B'].width = 40.0\n",
    "\n",
    "    # 写入表头\n",
    "    ws.cell(row=1, column=1).value = '原文本数据'\n",
    "    ws.cell(row=1, column=2).value = '标签数据（摘要）'\n",
    "    ws.cell(row=1, column=1).alignment = align\n",
    "    ws.cell(row=1, column=2).alignment = align\n",
    "\n",
    "    for i in range(len(posts)):\n",
    "        ws.cell(row=2+i, column=1).value = posts[i]\n",
    "        ws.cell(row=2+i, column=1).alignment = align\n",
    "        if(events[i] != 'None'):\n",
    "            ws.cell(row=2+i, column=2).value = events[i]\n",
    "        ws.cell(row=2+i, column=2).alignment = align\n",
    " \n",
    "    # 保存表格\n",
    "    wb.save(filename=output_file_name)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = list(zip(posts,events))\n",
    "random.shuffle(tp)\n",
    "posts = [i[0] for i in tp]\n",
    "events = [i[1] for i in tp]\n",
    "\n",
    "lens = len(posts)\n",
    "t = int(len(posts) / 5)\n",
    "names = ['李帅', '周云弈', '刘熠杨', '周芳妍', '刘天一']\n",
    "for i in range(5):\n",
    "    save_excel(posts[i*t:(i+1)*t], events[i*t:(i+1)*t], f'./dataset/text_summary/text_summary_{names[i]}.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通过模型生成文本摘要导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model=torch.load('./models/text_summary.model')\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Randeng-BART-139M-SUMMARY')\n",
    "\n",
    "def make_summary_from_execel(model, path):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    workbook = openpyxl.load_workbook(path)\n",
    "    table = workbook.active\n",
    "    rows = table.max_row\n",
    "\n",
    "    for row in tqdm(range(2, rows)):\n",
    "        text = table.cell(row, 1).value\n",
    "        inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "        res = tokenizer.decode(model.generate(inputs['input_ids'].to(device), max_length=128, do_sample=False)[0]).replace('</s>', '').strip()\n",
    "        table.cell(row, 2).value = res\n",
    "\n",
    "    workbook.save(path)\n",
    "    return True\n",
    "\n",
    "# make_summary_from_execel(model, './dataset/text_summary/self_summary/text_summary_李帅.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命名体识别数据导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_summary(root):\n",
    "    summary = []\n",
    "    for p in os.listdir(root):\n",
    "        workbook = openpyxl.load_workbook(os.path.join(root,p))\n",
    "        table = workbook.active\n",
    "        rows = table.max_row\n",
    "        for row in tqdm(range(2, rows+1)):\n",
    "            text = table.cell(row, 2).value\n",
    "            if(text and text.strip()!= ''):\n",
    "                summary.append(text)\n",
    "                \n",
    "        workbook.close()\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = get_summary('./dataset/text_summary/self_summary/labeled/')\n",
    "with open(\"./dataset/name_recognition/doccano/summary.txt\", \"w\", encoding='utf-8') as f:\n",
    "    for i in summary:\n",
    "        f.write(i + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从原数据到情感分类标签数据导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = 'cuda' if(torch.cuda.is_available()) else 'cpu'\n",
    "print(device)\n",
    "data = get_raw_data('./rawdata/weibo3.json')\n",
    "model = torch.load('./models/text_summary2.model').to(device)\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Randeng-BART-139M-SUMMARY')\n",
    "\n",
    "def make_comments(item, top=None):\n",
    "    inputs = tokenizer.encode_plus(item['post'], return_tensors='pt')\n",
    "    summary = tokenizer.decode(model.generate(inputs['input_ids'].to(device), max_length=128, do_sample=False)[0]).replace('</s>', '').strip()\n",
    "    comments = []\n",
    "    nums = 0\n",
    "    if(summary):\n",
    "        for comment in item['comments']:\n",
    "            comment = comment['content'].strip()\n",
    "            if(comment and comment != 'None'):\n",
    "                comment = summary + '###' + comment\n",
    "                comments.append(comment)\n",
    "                nums += 1\n",
    "            if(top and nums >= top):\n",
    "                break\n",
    "\n",
    "    return comments\n",
    "\n",
    "outputs = []\n",
    "for key in data:\n",
    "    column = data[key]\n",
    "    random.shuffle(column)\n",
    "    for item in tqdm(column[:50]):  # 每个栏目随机选50条微博\n",
    "        comments = make_comments(item, top=20)  # 取前20条评论\n",
    "        if(len(comments) > 0):\n",
    "            outputs += comments\n",
    "            \n",
    "outputs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/attitude_classify/doccano/commtens4.txt', 'w') as f:\n",
    "    for line in outputs:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从原数据到命名体识别标签数据导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = 'cuda' if(torch.cuda.is_available()) else 'cpu'\n",
    "print(device)\n",
    "data = get_raw_data('./rawdata/weibo3.json')\n",
    "model = torch.load('./models/text_summary2.model').to(device)\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Randeng-BART-139M-SUMMARY')\n",
    "\n",
    "def generate_summary(item, top=None):\n",
    "    inputs = tokenizer.encode_plus(item['post'], return_tensors='pt')\n",
    "    summary = tokenizer.decode(model.generate(inputs['input_ids'].to(device), max_length=128, do_sample=False)[0]).replace('</s>', '').strip()\n",
    "    if(summary):\n",
    "        return [summary]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "outputs = []\n",
    "for key in data:\n",
    "    column = data[key]\n",
    "    random.shuffle(column)\n",
    "    for item in tqdm(column[:50]):  # 每个栏目随机选50条微博\n",
    "        summary = generate_summary(item)\n",
    "        if(len(summary) > 0):\n",
    "            outputs += summary\n",
    "            \n",
    "outputs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/name_recognition/doccano/summary2.txt', 'w') as f:\n",
    "    for line in outputs:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命名体识别jsonl数据处理触发词动作同时出现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_repeat(item):\n",
    "    trigger_b = trigger_e = -1\n",
    "    for i in item['label']:\n",
    "        if(i[2] == '触发词'):\n",
    "            trigger_b = i[0]\n",
    "            trigger_e = i[1]\n",
    "    if(trigger_b!= -1 and trigger_e!= -1):\n",
    "        idx = -1\n",
    "        for n, i in enumerate(item['label']):\n",
    "            if(i[0] == trigger_b and i[1] == trigger_e and i[2] != '触发词'):\n",
    "                idx = n\n",
    "        item['label'].pop(idx)\n",
    "    return item\n",
    "\n",
    "\n",
    "path = \"./dataset/name_recognition/all.jsonl\"\n",
    "\n",
    "# 读文件并处理\n",
    "data = []\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line_js = json.loads(line)\n",
    "        line_js = remove_repeat(line_js)\n",
    "        data.append(json.dumps(line_js, ensure_ascii=False))\n",
    "        \n",
    "# 写回\n",
    "with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in data:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用ChatGPT生成文本摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "from tqdm import tqdm \n",
    "openai.organization = \"org-CElVwr63ZPcLedUtUJJ7XmTn\"\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取原数据\n",
    "with open('./rawdata/weibo_3_4.json', 'r', encoding=\"utf-8\") as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "sentence = []\n",
    "for key in data:\n",
    "    for d in data[key]:\n",
    "        if(512 > len(d['post']) > 20):\n",
    "            sentence.append(d['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 1450/1953 [2:04:54<49:32,  5.91s/it]  "
     ]
    }
   ],
   "source": [
    "# 生成摘要\n",
    "result = []\n",
    "for s in tqdm(sentence):\n",
    "  message = '请按照以下格式生成10-15字的摘要，且保证文本原意不变\\n文本：机场航站楼门口偶遇鞠婧祎鞠婧祎这腰比我腿都细本人真的好美！！\\n摘要：机场偶遇鞠婧祎\\n文本：{}\\n摘要：'.format(s)\n",
    "  response = openai.ChatCompletion.create(**{\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": message}]\n",
    "  })\n",
    "  d = {\n",
    "    'title': response['choices'][0]['message']['content'].strip(),\n",
    "    'content': s\n",
    "  }\n",
    "  result.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/text_summary/ChatGPT/summary.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用ChatGPT生成引导建议"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = '去马尔代夫看世界'\n",
    "attitude = '厌烦'\n",
    "message = \"\"\"请针对事件及其引起的心态变化，分点给出80-100字的引导建议。示例如下：\n",
    "事件：机场偶遇鞠婧祎。\n",
    "心态：高兴。\n",
    "引导建议：\n",
    "1.鼓励积极分享：鼓励大家积极分享彼此的喜悦和兴奋，倾听他人的故事和经历，营造一个积极向上的氛围。\n",
    "2.强调尊重和礼貌：提醒大家要尊重偶像和其他旅客的隐私和个人空间，避免对他们造成不必要的困扰和不安全因素。\n",
    "3.提倡感恩和珍惜：引导大家感恩这个难得的机会，并提倡珍惜这个美好的时刻，用心记录和回忆这个经历。\n",
    "4.引导理性思考：提醒大家要保持冷静和理智，避免过度热衷或表现过度，理性思考自己的行为举止是否适当。\n",
    "现在，请按照上述格式针对下面事件及其引起的心态变化补全引导建议，且字数在80-100字左右。\n",
    "事件：{}。\n",
    "心态：{}。\n",
    "引导建议：\"\"\".format(event, attitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beatsleo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16ea64f9ee948d927ad35fd9dd41586a042d593dc7bf73dbea6b47fb27e81f20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
