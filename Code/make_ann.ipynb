{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import openpyxl\n",
    "from openpyxl.styles import Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data(path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "data = get_raw_data('./rawdata/weibo3.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本摘要数据导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "posts = []\n",
    "for i in data:\n",
    "    for d in data[i]:\n",
    "        events.append(d['event'])\n",
    "        posts.append(d['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_excel(posts, events, output_file_name):\n",
    "    \"\"\"\n",
    "    将数据写入xlsx文件\n",
    "    \"\"\"\n",
    "    if not output_file_name.endswith('.xlsx'):\n",
    "        output_file_name += '.xlsx'\n",
    " \n",
    "    # 创建一个workbook对象，而且会在workbook中至少创建一个表worksheet\n",
    "    wb = openpyxl.Workbook()\n",
    "    # 获取当前活跃的worksheet,默认就是第一个worksheet\n",
    "    ws = wb.active\n",
    "    align = Alignment(horizontal='center', vertical='center', wrap_text=True)\n",
    "    ws.column_dimensions['A'].width = 40.0\n",
    "    ws.column_dimensions['B'].width = 40.0\n",
    "\n",
    "    # 写入表头\n",
    "    ws.cell(row=1, column=1).value = '原文本数据'\n",
    "    ws.cell(row=1, column=2).value = '标签数据（摘要）'\n",
    "    ws.cell(row=1, column=1).alignment = align\n",
    "    ws.cell(row=1, column=2).alignment = align\n",
    "\n",
    "    for i in range(len(posts)):\n",
    "        ws.cell(row=2+i, column=1).value = posts[i]\n",
    "        ws.cell(row=2+i, column=1).alignment = align\n",
    "        if(events[i] != 'None'):\n",
    "            ws.cell(row=2+i, column=2).value = events[i]\n",
    "        ws.cell(row=2+i, column=2).alignment = align\n",
    " \n",
    "    # 保存表格\n",
    "    wb.save(filename=output_file_name)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = list(zip(posts,events))\n",
    "random.shuffle(tp)\n",
    "posts = [i[0] for i in tp]\n",
    "events = [i[1] for i in tp]\n",
    "\n",
    "lens = len(posts)\n",
    "t = int(len(posts) / 5)\n",
    "names = ['李帅', '周云弈', '刘熠杨', '周芳妍', '刘天一']\n",
    "for i in range(5):\n",
    "    save_excel(posts[i*t:(i+1)*t], events[i*t:(i+1)*t], f'./dataset/text_summary/text_summary_{names[i]}.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通过模型生成文本摘要导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model=torch.load('./models/text_summary.model')\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Randeng-BART-139M-SUMMARY')\n",
    "\n",
    "def make_summary_from_execel(model, path):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    workbook = openpyxl.load_workbook(path)\n",
    "    table = workbook.active\n",
    "    rows = table.max_row\n",
    "\n",
    "    for row in tqdm(range(2, rows)):\n",
    "        text = table.cell(row, 1).value\n",
    "        inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "        res = tokenizer.decode(model.generate(inputs['input_ids'].to(device), max_length=128, do_sample=False)[0]).replace('</s>', '').strip()\n",
    "        table.cell(row, 2).value = res\n",
    "\n",
    "    workbook.save(path)\n",
    "    return True\n",
    "\n",
    "# make_summary_from_execel(model, './dataset/text_summary/self_summary/text_summary_李帅.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命名体识别数据导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_summary(root):\n",
    "    summary = []\n",
    "    for p in os.listdir(root):\n",
    "        workbook = openpyxl.load_workbook(os.path.join(root,p))\n",
    "        table = workbook.active\n",
    "        rows = table.max_row\n",
    "        for row in tqdm(range(2, rows+1)):\n",
    "            text = table.cell(row, 2).value\n",
    "            if(text and text.strip()!= ''):\n",
    "                summary.append(text)\n",
    "                \n",
    "        workbook.close()\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = get_summary('./dataset/text_summary/self_summary/labeled/')\n",
    "with open(\"./dataset/name_recognition/doccano/summary.txt\", \"w\", encoding='utf-8') as f:\n",
    "    for i in summary:\n",
    "        f.write(i + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从原数据到情感分类标签数据导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = 'cuda' if(torch.cuda.is_available()) else 'cpu'\n",
    "print(device)\n",
    "data = get_raw_data('./rawdata/weibo3.json')\n",
    "model = torch.load('./models/text_summary2.model').to(device)\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Randeng-BART-139M-SUMMARY')\n",
    "\n",
    "def make_comments(item, top=None):\n",
    "    inputs = tokenizer.encode_plus(item['post'], return_tensors='pt')\n",
    "    summary = tokenizer.decode(model.generate(inputs['input_ids'].to(device), max_length=128, do_sample=False)[0]).replace('</s>', '').strip()\n",
    "    comments = []\n",
    "    nums = 0\n",
    "    if(summary):\n",
    "        for comment in item['comments']:\n",
    "            comment = comment['content'].strip()\n",
    "            if(comment and comment != 'None'):\n",
    "                comment = summary + '###' + comment\n",
    "                comments.append(comment)\n",
    "                nums += 1\n",
    "            if(top and nums >= top):\n",
    "                break\n",
    "\n",
    "    return comments\n",
    "\n",
    "outputs = []\n",
    "for key in data:\n",
    "    column = data[key]\n",
    "    random.shuffle(column)\n",
    "    for item in tqdm(column[:50]):  # 每个栏目随机选50条微博\n",
    "        comments = make_comments(item, top=20)  # 取前20条评论\n",
    "        if(len(comments) > 0):\n",
    "            outputs += comments\n",
    "            \n",
    "outputs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/attitude_classify/doccano/commtens4.txt', 'w') as f:\n",
    "    for line in outputs:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从原数据到命名体识别标签数据导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:11<00:00,  4.42it/s]\n",
      "100%|██████████| 50/50 [00:10<00:00,  4.95it/s]\n",
      "100%|██████████| 50/50 [00:09<00:00,  5.20it/s]\n",
      "100%|██████████| 50/50 [00:10<00:00,  4.82it/s]\n",
      " 92%|█████████▏| 46/50 [00:09<00:00,  4.06it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = 'cuda' if(torch.cuda.is_available()) else 'cpu'\n",
    "print(device)\n",
    "data = get_raw_data('./rawdata/weibo3.json')\n",
    "model = torch.load('./models/text_summary2.model').to(device)\n",
    "tokenizer=AutoTokenizer.from_pretrained('IDEA-CCNL/Randeng-BART-139M-SUMMARY')\n",
    "\n",
    "def generate_summary(item, top=None):\n",
    "    inputs = tokenizer.encode_plus(item['post'], return_tensors='pt')\n",
    "    summary = tokenizer.decode(model.generate(inputs['input_ids'].to(device), max_length=128, do_sample=False)[0]).replace('</s>', '').strip()\n",
    "    if(summary):\n",
    "        return [summary]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "outputs = []\n",
    "for key in data:\n",
    "    column = data[key]\n",
    "    random.shuffle(column)\n",
    "    for item in tqdm(column[:50]):  # 每个栏目随机选50条微博\n",
    "        summary = generate_summary(item)\n",
    "        if(len(summary) > 0):\n",
    "            outputs += summary\n",
    "            \n",
    "outputs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/name_recognition/doccano/summary2.txt', 'w') as f:\n",
    "    for line in outputs:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命名体识别jsonl数据处理触发词动作同时出现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_repeat(item):\n",
    "    trigger_b = trigger_e = -1\n",
    "    for i in item['label']:\n",
    "        if(i[2] == '触发词'):\n",
    "            trigger_b = i[0]\n",
    "            trigger_e = i[1]\n",
    "    if(trigger_b!= -1 and trigger_e!= -1):\n",
    "        idx = -1\n",
    "        for n, i in enumerate(item['label']):\n",
    "            if(i[0] == trigger_b and i[1] == trigger_e and i[2] != '触发词'):\n",
    "                idx = n\n",
    "        item['label'].pop(idx)\n",
    "    return item\n",
    "\n",
    "\n",
    "path = \"./dataset/name_recognition/all.jsonl\"\n",
    "\n",
    "# 读文件并处理\n",
    "data = []\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line_js = json.loads(line)\n",
    "        line_js = remove_repeat(line_js)\n",
    "        data.append(json.dumps(line_js, ensure_ascii=False))\n",
    "        \n",
    "# 写回\n",
    "with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in data:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beatsleo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16ea64f9ee948d927ad35fd9dd41586a042d593dc7bf73dbea6b47fb27e81f20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
