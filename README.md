# 需求分析
主要任务：完成微博、抖音和Bilibili平台的爬虫

> * [ ] Scrapy框架与Selenium库的学习（预计1周）
>
>   * [ ] Scrapy的基本使用
>   * [ ] Selenium的基本使用
>   * [ ] 边学边写，能以本项目要爬的平台为案例最好
>
> * [ ] 微博平台的爬取与数据清洗（预计1周）
>
>   * [ ] 以dict的形式存储数据
>
>     ```python
>     data = {
>         'event': None, # 该条帖子的核心事件，以 #xxx# 的tag格式引出，若有多条，随机选择一条或选择第一条，若没有则为None
>         'post': 'xxx'	# 帖子的内容
>         'time': 'xxxx-xx-xx',	# 发布时间，以给出格式存储
>         'comments': {	# 评论，以嵌套dict的形式存储
>             'content': 'xxx'	# 评论内容
>             'time': 'xxxx-xx-xx',	# 评论时间，以给出格式存储
>             'ip': 'xxx'	# 评论发布者的IP位置信息
>         }
>     }
>     ```
>
>   * [ ] 从微博的热门、社会、科技、电影、音乐、数码、汽车、游戏里面爬取数据，只爬点赞数量超过100或评论数量超过20的。
>
>   * [ ] 将所有图片过滤掉，只保留文字。
>
>   * [ ] 尽可能爬取多的帖子和评论，对反爬手段采取合适的措施
>
>   * [ ] 暂存本地
>
> * [ ] 抖音平台的爬取与数据清洗（预计1周）
>
>   * [ ] 以dict的形式存储数据
>
>     `data数据格式同上，其中'event'的tag格式为 #xxx ，在部分视频中会有xx榜或xx热点的提示，如果存在优先选择提示中的内容作为event(class="wKeF4l1I")，帖子内容为视频标题`
>
>   * [ ] 从抖音的热点、游戏、娱乐、二次元、音乐、美食、知识、体育、时尚里面爬取数据，只爬点赞数量超过100或评论数量超过20的。
>
>   * [ ] 将所有图片过滤掉，只保留文字。
>
>   * [ ] 暂存本地
>
> * [ ] Bilibili平台的爬取与数据清洗（预计1周）
>
>   * [ ] 以dict的形式存储数据
>   * [ ] 
>
> * [ ] 连接数据库，设置定时器，实现对三个平台的定时并行爬取（预计1周）
>
>   * [ ] 并行爬取的实现
>   * [ ] 大概每1或2小时爬一次
>   * [ ] 将保存部分从本地改为存储在服务器数据库中，具体格式见周云弈部分。
